#### Вопрос 07

##### Теорема кодирования Шеннона для ИБП

*(из Википедии)*

Теорема Шеннона для ИБП связывают энтропию источника и возможность сжатия кодированием с потерями и последующим неоднозначным декодированием. 

Прямая теорем показывает, что с помощью кодирования с потерями возможно достичь степени сжатия
$$
\frac{N}{L}\approx\frac{H(U)(1+\varepsilon)}{log_2D}
$$
сколь угодно близкой к энтропии источника, но все же больше последней. Обратная показывает, что лучший результат не достижим.

U - некоторый источник сообщений

H(U) - энтропия источника

N - длина сообщения после кодирования

L - длина сообщения до кодирования *(?)*

D - мощность алфавита кодера

*(умная формулировка)*

Для источника без памяти *U* с энтропией *H(U)* и любого $\varepsilon>0$ существует последовательность множеств однозначно декодирования $M_L$ мощности $2^{L(1+\varepsilon)H(U)}$ такая, что вероятность множества неоднозначного декодирования стремится к нулю $P(M^C_L)\rightarrow0$ при увеличении длины блока $L\rightarrow\infin$. Другими словами, сжатие возможно.